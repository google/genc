{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goy6i-1KOaAY"
      },
      "source": [
        "# Tutorial 1. Simple chain in LangChain powered by a device-to-Cloud model cascade\n",
        "\n",
        "This tutorial shows how to define simple application logic in LangChain, use our\n",
        "interop APIs to configure it to be powered by a cascade of models that spans\n",
        "across a model in Cloud and an on-device model on Android, and deploy it in a\n",
        "Java app on an Android phone. This illustrates many of the key interoperability\n",
        "and portability benefits of GenC in one concise package. See the follow-up\n",
        "tutorials listed in the parent directory for how you can further extend and\n",
        "customize such logic to power more complex use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr8gU1bxnMjA"
      },
      "source": [
        "## Initial setup\n",
        "\n",
        "Before we begin, we need to setup your environment, such that you can continue\n",
        "with the rest of this tutorial undisrupted.\n",
        "\n",
        "*   First, you need to start a Jupyter notebook with the GenC dependency\n",
        "    wired-in, and connect to that notebook - see\n",
        "    [SETUP.md](https://github.com/google/generative_computing/tree/master/SETUP.md)\n",
        "    at the root of the repo, and the supporting files in the\n",
        "    [Jupyter setup directory](https://github.com/google/generative_computing/tree/master/generative_computing/docs/tutorials/jupyter_setup/)\n",
        "    for instructions how to setup the build and run environment and get Jupyter\n",
        "    up and running.\n",
        "\n",
        "*   Next, you need to setup access to the Gemini Pro model that will be used\n",
        "    in the tutorials. Please see the\n",
        "    [instructions](https://ai.google.dev/tutorials/rest_quickstart)\n",
        "    on how to get an API key to access this model through Google AI Studio.\n",
        "\n",
        "*   Finally, for the last portion of the tutorial that includes deployment on\n",
        "    Android, you will need to obtain a local model for your device. Please see\n",
        "    [models.md](https://github.com/google/generative_computing/tree/master/generative_computing/docs/models.md)\n",
        "    for information on how to obtain models and\n",
        "    what backends to use. This tutorial supports running your model using\n",
        "    MediaPipe (optimized GPU performance, but a limited set of models) or\n",
        "    LlamaCpp (CPU-only, but many models supported). Once you have your model,\n",
        "    you'll need to push the model file to your phone. Keep note of the path\n",
        "    you push it to as you'll need to include this when you construct your IR\n",
        "    below.\n",
        "\n",
        "    For example, for a MediaPipe Gemma model:\n",
        "    ```\n",
        "    adb push {{src-model-dir}}/gemma-2b-it-gpu-int4.bin /data/local/tmp/llm/gemma-2b-it-gpu-int4.bin\n",
        "    ```\n",
        "\n",
        "    For a LlamaCpp model [e.g. Gemma 2B Quantized model](https://huggingface.co/lmstudio-ai/gemma-2b-it-GGUF/tree/main):\n",
        "    ```\n",
        "    adb push {{src-model-dir}}/gemma-2b-it-q4_k_m.gguf /data/local/tmp/llm/gemma-2b-it-q4_k_m.gguf\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A67WrPezqN2z"
      },
      "source": [
        "Now, to verify that GenC dependencies are loaded correctly, let's run a bunch\n",
        "of imports we're going to use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpMQqnD3qKIz"
      },
      "outputs": [],
      "source": [
        "import generative_computing as genc\n",
        "from generative_computing import authoring\n",
        "from generative_computing import interop\n",
        "from generative_computing import runtime\n",
        "from generative_computing import examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Q-Cp9gorfw"
      },
      "source": [
        "## Defining application logic in LangChain\n",
        "\n",
        "We're going to create here an example application logic using LangChain APIs.\n",
        "For the sake of simplicy, let's go with a simple chain that consists of a prompt\n",
        "template feeding into an LLM call. Let's define it as a function, so that we can\n",
        "later play with different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOihL6fbpTE8"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def create_my_chain(llm):\n",
        "  return langchain.chains.LLMChain(\n",
        "      llm=llm,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"topic\"],\n",
        "          template=\"Tell me about {topic}?\",\n",
        "      ),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Sr0EXyrEwf"
      },
      "source": [
        "## Declaring a Cloud model you will use to power the chain\n",
        "\n",
        "Now, let's define a model we can use. In GenC, we refer to models symbolically\n",
        "since the same model may be provisioned differently depending on where you run\n",
        "the code (recall that we want to demonstrate in this tutorial is running your\n",
        "application logic in colab first, but then porting it to run on a phone).\n",
        "To facilitate this, GenC provides interop APIs that enable you to declare the\n",
        "use of a model, e.g., as shown below. For this tutorial, we're going to use\n",
        "the Gemini Pro model from Google Studio AI.\n",
        "\n",
        "NOTE: Please make sure you have an API_KEY to use as covered in the \"Initial setup\" section (above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPTN8y8gHSvh"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.interop.gemini.create_config(API_KEY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beCPCSZCR-JO"
      },
      "source": [
        "Now, you can construct the chain with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlPZJSaNSTtr"
      },
      "outputs": [],
      "source": [
        "my_chain = create_my_chain(my_cloud_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4M8y-PYTbzE"
      },
      "source": [
        "## Generating portable intermediate representation (IR)\n",
        "\n",
        "Now that you have the application logic (the chain you defined above), we need\n",
        "to translate it into what we call a *portable intermediate representation* (IR\n",
        "for short) that can be deployed on an Android phone. You do this by calling the\n",
        "converstion function provided by GenC, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfxHpZ6Qsqcg"
      },
      "outputs": [],
      "source": [
        "my_portable_ir = genc.interop.langchain.create_computation(my_chain)\n",
        "print(my_portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRszCjdZTwqT"
      },
      "source": [
        "At the time of this writing, this converter only supports a subset of LangChain\n",
        "functionality; we'll work to augment the coverage over time (and we welcome your\n",
        "help if there's a feature of LangChain you'd like to see covered and are willing\n",
        "to contribute it to the platform)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5sTduPZUHyt"
      },
      "source": [
        "## Testing the IR locally in the Colab environment\n",
        "\n",
        "Before we move over to deployment on Android, let's first test that the IR is\n",
        "indeed working. While our goal is to run it on-device, we can just as well run\n",
        "it here, in the colab environment (remember, all the code is portable). To do\n",
        "this, we first need to construct a runtime instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgImBVLEUcGg"
      },
      "outputs": [],
      "source": [
        "my_runtime = genc.examples.executor.create_default_executor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD3KJhT_Udhc"
      },
      "source": [
        "Now, the constructor above is provided for convenience in running the examples\n",
        "and tutorials, and is configured with a number of runtime capabilities that we\n",
        "use in this context. Runtimes in GenC are fully modular and configurable, and\n",
        "in most advanced uses, you'll want to configure a runtime that suits the\n",
        "specific environment you want to run in, or your particular application (e.g.,\n",
        "with additional custom dependencies, or without certain dependencies you don't\n",
        "want in your environment). One of the tutorials later in the sequence explains\n",
        "how to do that. For now, the default example runtime will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RUATRQOVJq5"
      },
      "source": [
        "Given the runtime and the portable IR we want to run, we can construct a\n",
        "*runner* object that will act like an ordinary Python function, and can\n",
        "be directly invoked, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwFnhO-ns3wt"
      },
      "outputs": [],
      "source": [
        "my_runner = genc.runtime.Runner(my_portable_ir, my_runtime)\n",
        "\n",
        "print(my_runner(\"scuba diving\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArWgVKTn_hzC"
      },
      "source": [
        "Because of the portability of the IR, at this point you could deploy this IR\n",
        "as-is to your Android phone, and test it there without the need for any changes,\n",
        "running the query with just the cloud model. One of GenC's goals is to enable\n",
        "easy experimentation and an iterative style of development, where you can check\n",
        "the results of your work at each step. To do this, jump to the section below\n",
        "entitled \"Saving the IR to a file for deployment to phone\" for instructions on\n",
        "deployment.\n",
        "\n",
        "Otherwise, if you'd rather prefer to keep expanding on the existing logic, and\n",
        "deploy the final version of the IR just once at the end, continue on to the next\n",
        "section, where you will add a device model and run the complete model cascade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0cGw1xSiRHw"
      },
      "source": [
        "## Adding an on-device model to form a model cascade\n",
        "\n",
        "Now, recall that what we promised to demonstrate in this tutorial running your\n",
        "application logic on a phone, where it might be powered by an on-device LLM\n",
        "while the the phone may be offline. To achieve this, we're going to need to\n",
        "modify the IR to include the on-device model. First, let's declare the use of\n",
        "an on-device model in LangChain, similarly to how we did above for the cloud\n",
        "model.\n",
        "\n",
        "NOTE: Make sure you have downloaded the on-device model on Android as covered\n",
        "in the \"Initial setup\" section above.\n",
        "You'll need to provide the backend and model_path below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xay1QiN6iRHw"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "class LocalBackend(Enum):\n",
        "  MEDIAPIPE = 1\n",
        "  LLAMACPP = 2\n",
        "\n",
        "# Change these values based on your desired backend and model\n",
        "BACKEND = LocalBackend.MEDIAPIPE\n",
        "MODEL_PATH = \"/data/local/tmp/llm/gemma-2b-it-gpu-int4.bin\"\n",
        "\n",
        "# Create IR for on device model\n",
        "if (BACKEND == LocalBackend.MEDIAPIPE):\n",
        "  my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "      uri = \"/device/llm_inference\",\n",
        "      config = {\"model_path\": MODEL_PATH,\n",
        "                \"max_tokens\": 64,\n",
        "                \"top_k\": 40,\n",
        "                \"temperature\": 0.8,\n",
        "                \"random_seed\": 100})\n",
        "elif (BACKEND == LocalBackend.LLAMACPP):\n",
        "    my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "      uri = \"/device/llamacpp\",\n",
        "      config = {\"model_path\" : MODEL_PATH,\n",
        "                \"num_threads\" : 4,\n",
        "                \"max_tokens\" : 64})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jlOValUiRHw"
      },
      "source": [
        "Now, we're going to combine the cloud and on-device models into a simple type\n",
        "of model cascade that spans across cloud and on-device LLMs. For simplicity's\n",
        "sake, let's define a two-model cascade that first tries to hit a cloud backend\n",
        "in case we're online, and that defaults to the use of an on-device model when offline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GbfpvMjiRHw"
      },
      "outputs": [],
      "source": [
        "my_model_cascade = genc.interop.langchain.ModelCascade(models=[\n",
        "    my_cloud_model, my_on_device_model])\n",
        "\n",
        "my_chain = create_my_chain(my_model_cascade)\n",
        "my_portable_ir = genc.interop.langchain.create_computation(my_chain)\n",
        "print(my_portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRpYfYPMiRHw"
      },
      "source": [
        "You could've chosen to order models in the cascade differently to achieve a\n",
        "different behavior. Everything is customizable! In the next tutorial in the\n",
        "sequence, we'll show you how you can construct an even more powerful routing\n",
        "mechanism, where routing is based on query sensitivity. For now, this simple\n",
        "cascade will suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYTmy5Y7Vdvz"
      },
      "source": [
        "## Saving the IR to a file for deployment to phone\n",
        "\n",
        "Now that you tested the IR locally, it's time to deploy it on your phone and\n",
        "test it there. First, let's save the IR into a file on the local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njam4ZuNoiJ6"
      },
      "outputs": [],
      "source": [
        "with open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(my_portable_ir.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLV1BlXt0fK"
      },
      "source": [
        "## Install Generative Computing Demo app, deploy the IR file on the phone\n",
        "\n",
        "### Building/Deploying the Android app in Docker\n",
        "\n",
        "If you're already running this notebook inside of a docker container and\n",
        "followed the steps in\n",
        "[SETUP.md](https://github.com/google/generative_computing/tree/master/SETUP.md)\n",
        "then you have an Android\n",
        "build enviroment already setup. You can build the APK directly from the root\n",
        "directory (`/generative_computing` if the command from SETUP.md was used).\n",
        "\n",
        "```\n",
        "bazel build --config=android_arm64 generative_computing/java/src/java/org/generativecomputing/examples/apps/gencdemo:app\n",
        "```\n",
        "\n",
        "Once built, the APK will be in the `bazel-bin` folder. This will need to be\n",
        "copied to the folder that is shared between the Docker container and the host\n",
        "machine (because Docker doesn't have access to your USB devices).\n",
        "If you followed the command in\n",
        "[SETUP.md](https://github.com/google/generative_computing/tree/master/SETUP.md)\n",
        "then this folder is `/generative_computing`:\n",
        "\n",
        "```\n",
        "cp bazel-bin/generative_computing/java/src/java/org/generativecomputing/examples/apps/gencdemo/app.apk /generative_computing\n",
        "cp /tmp/genc_demo.pb /generative_computing\n",
        "```\n",
        "\n",
        "In a different terminal outside of the Docker container, you can now ADB\n",
        "install the APK and push the IR.\n",
        "\n",
        "```\n",
        "adb install app.apk\n",
        "adb push genc_demo.pb /data/local/tmp/genc_demo.pb\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtmVl-WHtF_T"
      },
      "source": [
        "## Run the demo on phone\n",
        "\n",
        "Make sure your device is connected to the Internet.\n",
        "\n",
        "1. Open the “Generative Computing Demo” app and type a topic in the UI. As a reminder, here is the prompt template we are using: \"Tell me about {topic}?\"\n",
        "  * Example text to enter in UI: \"scuba diving\"\n",
        "\n",
        "2. See the result. This is the result coming from the Cloud model (make sure\n",
        "that the internet connection is available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iLVUCbRaLMD"
      },
      "source": [
        "Now, you may want to play with setting the Airplane Mode on your Android phone\n",
        "to On and Off. As you do that, retry the query, and notice how the result\n",
        "changes depending on the model in use (responses from the on-device and cloud\n",
        "models tend to look differently). Under the hood, the cascade you defined\n",
        "prompts GenC to first try the cloud model, but if it fails (while in airplane\n",
        "mode), it simply falls back to the on-device model (if present)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
