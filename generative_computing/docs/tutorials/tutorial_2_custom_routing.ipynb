{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUKRoMbYOfWz"
      },
      "source": [
        "# Tutorial 2: Custom Routing\n",
        "\n",
        "In the preceding turorial, you have seen en example of a single model cascade\n",
        "that spans across on-device and Cloud, and uses model availability to determine\n",
        "which model to use. Here, we expand on this and explore a more elabprate setup,\n",
        "where the decision which mdoel to use is based on the query. We first use the\n",
        "on-device model to score the sensitivity of the input text, and depending on\n",
        "that, we route it to the cloud model (if the query is sensitive, and thus needs\n",
        "a more powerful model to process), or the on-device model (if the query appears\n",
        "to be simple enough)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgiLioAo28Lj"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "Before proceeding, please follow the\n",
        "[instructions](.../tutorial_1_simple_cascade.ipynb) in Tutorial 1 to set up your environment, connect Jupyter, and run the command below to run\n",
        "the GenC imports we're goign to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqxX6-wNVmrF"
      },
      "outputs": [],
      "source": [
        "import generative_computing.python as genc\n",
        "from generative_computing.python import authoring\n",
        "from generative_computing.python import interop\n",
        "from generative_computing.python import runtime\n",
        "from generative_computing.python.examples import executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE6p3kjYPsGD"
      },
      "source": [
        "## Define a two model routing\n",
        "\n",
        "First, let's define the on-device and cloud models we will be using, in a manner\n",
        "similar to how we've done this in the first tutorial. We define them in\n",
        "LangChain as before, but this time, we convert them to the IR form right away,\n",
        "becuase in the remainder of this tutorial we're going to use some of GenC's\n",
        "native API for composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CbNgN_gCBAx"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"/data/local/tmp/llm/model_gpu.tflite\"  #@param\n",
        "\n",
        "my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/device/llm_inference\",\n",
        "    config={\"model_path\": MODEL_PATH})\n",
        "\n",
        "my_on_device_model_ir = genc.interop.langchain.create_computation(my_on_device_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1_1B8uqCLd1"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.interop.gemini.create_config(API_KEY))\n",
        "\n",
        "my_cloud_model_ir = genc.interop.langchain.create_computation(my_cloud_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfSK2A9kb5m"
      },
      "source": [
        "Now, let's define the scorer model. Here, for demo purposes, a simple few-shot\n",
        "prompt template is used to assess political sensitivity. One can replace this\n",
        "with more advanced scorer models. The scorer uses the on-device model to decide\n",
        "on sensitivity (so we can avoid hitting the cloud backends and thus potentially\n",
        "reduce the response time if the query is simple enough)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1pc4xN5PD_7"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        " Instructions: The following are questions that can touch on sensitive or\n",
        " political topics. Please return True or False boolean with no explanation\n",
        " if the question touches on sensitive or political topic. Q: what are your\n",
        " views on democracy and communism? A: True Q: what are your views on\n",
        " current ruling party in US? A: True Q: What is the weather today? A: False\n",
        " Q: {query} A:\n",
        "\"\"\"\n",
        "\n",
        "scorer_model = langchain.chains.LLMChain(\n",
        "      llm=my_on_device_model,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"query\"],\n",
        "          template=prompt_template)\n",
        "      )\n",
        "\n",
        "regex_match = (\n",
        "    genc.authoring.create_regex_partial_match(\"A: True|A: true|true|True\")\n",
        ")\n",
        "\n",
        "scorer_chain = (\n",
        "    genc.interop.langchain.CustomChain()\n",
        "    | scorer_model\n",
        "    | regex_match\n",
        "  )\n",
        "\n",
        "scorer_chain_ir = genc.interop.langchain.create_computation(scorer_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ebql2lF8ZG"
      },
      "source": [
        "Now, it's time to put all these pieces togethr into a whole. As noted before,\n",
        "we're going to illustrate here the use of GenC's conditional expression (one of\n",
        "the supplied operators) for composition. The expression first uses the scorer\n",
        "logic we defined just above, and depending on the outcome, passes control to\n",
        "either of the two models for valuation.\n",
        "\n",
        "Once again, we generate th IR, so that we can deploy the result or Android."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5FjkSQbGY8h"
      },
      "outputs": [],
      "source": [
        "portable_ir = genc.authoring.create_lambda_from_fn(\n",
        "    \"x\",\n",
        "    lambda arg: genc.authoring.create_conditional(\n",
        "        genc.authoring.create_call(scorer_chain_ir, arg),\n",
        "        genc.authoring.create_call(my_cloud_model_ir, arg),\n",
        "        genc.authoring.create_call(my_on_device_model_ir, arg),\n",
        "    ),\n",
        ")\n",
        "print(portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scf-MMnkPoU9"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixoPrcWEf9rs"
      },
      "source": [
        "TODO - this will fail because the on-device model we use is not yet linked in the colab environment; fix this or just switch to llamacpp as the provider in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kSazHPnQmwt"
      },
      "outputs": [],
      "source": [
        "runner = genc.runtime.Runner(portable_ir,\n",
        "                             genc.examples.executor.create_default_executor())\n",
        "\n",
        "runner(\"What are your views on ice cream?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cssG2tkPHc4c"
      },
      "source": [
        "## Save the IR to a file\n",
        "\n",
        "Run the following code to generate the a file containing the IR from above\n",
        "computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY1DlYqnJ9XI"
      },
      "outputs": [],
      "source": [
        "with open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(portable_ir.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fk_BJZ6K5V3"
      },
      "source": [
        "## Install Generative Computing Demo app and deploy IR file to phone\n",
        "\n",
        "Please see [\"Install Generative Computing Demo app and deploy the IR file to phone\"](.../tutorial_1_simple_cascade.ipynb) in Tutorial 1 for instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbkrAZ9boui"
      },
      "source": [
        "## Run the demo\n",
        "\n",
        "1. Open Generative Computing Demo app, and enter a sensitive query. See response.\n",
        "\n",
        "2. Next, enter a non-sensitive query. See response.\n",
        "\n",
        "3. Interesting observation: Notice the length of the text response for non-sensitive query compared to the sensitive query. Any guesses?\n",
        "  *  The non-sensitive query is routed to on-device model which in our demo setup returns shorter text responses to balance response time against lesser compute resources on device while the cloud model returns more verbose response.\n",
        "\n",
        "4. The intent of this custom routing example is to illustrate that developers can create custom routing policies to dynamically choose which model to use per\n",
        "different use-cases and model capabilities."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
