{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUKRoMbYOfWz"
      },
      "source": [
        "# Tutorial 2: Custom Routing\n",
        "\n",
        "In the preceding turorial, you have seen en example of a single model cascade\n",
        "that spans across on-device and Cloud, and uses model availability to determine\n",
        "which model to use. Here, we expand on this and explore a more elaborate setup,\n",
        "where the decision which model to use is based on the input query. We first use\n",
        "the on-device model to score the sensitivity of the input query, and depending\n",
        "on that, we route it to the cloud model (if the query is sensitive, and thus\n",
        "needs a more powerful model to process), or the on-device model (if the query\n",
        "appears to be simple enough for the on-device model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgiLioAo28Lj"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "Before proceeding, please follow the instructions in\n",
        "[Tutorial 1](https://github.com/google/generative_computing/tree/master/generative_computing/docs/tutorials/tutorial_1_simple_cascade.ipynb)\n",
        "to set up your environment, connect Jupyter, and run the command below to run\n",
        "the GenC imports we're going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqxX6-wNVmrF"
      },
      "outputs": [],
      "source": [
        "import generative_computing.python as genc\n",
        "from generative_computing.python import authoring\n",
        "from generative_computing.python import interop\n",
        "from generative_computing.python import runtime\n",
        "from generative_computing.python.examples import executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE6p3kjYPsGD"
      },
      "source": [
        "## Define a two model routing\n",
        "\n",
        "First, let's define the on-device and cloud models we will be using, in a manner\n",
        "similar to how we've done this in the first tutorial. We define them in\n",
        "LangChain as before, but this time, we convert them to the IR form right away,\n",
        "becuase in the remainder of this tutorial we're going to use some of GenC's\n",
        "native API for composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CbNgN_gCBAx"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"/data/local/tmp/llm/gemma-2b-it-gpu-int4.bin\"  #@param\n",
        "\n",
        "my_on_device_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/device/llm_inference\",\n",
        "    config={\"model_path\": MODEL_PATH})\n",
        "\n",
        "my_on_device_model_ir = genc.interop.langchain.create_computation(my_on_device_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1_1B8uqCLd1"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\"  #@param\n",
        "\n",
        "my_cloud_model = genc.interop.langchain.CustomModel(\n",
        "    uri=\"/cloud/gemini\",\n",
        "    config=genc.interop.gemini.create_config(API_KEY))\n",
        "\n",
        "my_cloud_model_ir = genc.interop.langchain.create_computation(my_cloud_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfSK2A9kb5m"
      },
      "source": [
        "Now, let's define the scorer chain that will be used to assess the sensitivity\n",
        "of the inupt query. Here, for demo purposes, a simple few-shot prompt template\n",
        "is used to assess political sensitivity. One can replace this with more\n",
        "advanced scorer models. The scorer uses the on-device model to decide on\n",
        "sensitivity (so we can avoid hitting the cloud backends and thus potentially\n",
        "reduce the response time if the query is simple enough)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1pc4xN5PD_7"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        " Instructions: The following are questions that can touch on sensitive or\n",
        " political topics. Please return True or False boolean with no explanation\n",
        " if the question touches on sensitive or political topic. Q: what are your\n",
        " views on democracy and communism? A: True Q: what are your views on\n",
        " current ruling party in US? A: True Q: What is the weather today? A: False\n",
        " Q: {query} A:\n",
        "\"\"\"\n",
        "\n",
        "scorer_model = langchain.chains.LLMChain(\n",
        "      llm=my_on_device_model,\n",
        "      prompt=PromptTemplate(\n",
        "          input_variables=[\"query\"],\n",
        "          template=prompt_template)\n",
        "      )\n",
        "\n",
        "regex_match = (\n",
        "    genc.authoring.create_regex_partial_match(\"A: True|A: true|true|True\")\n",
        ")\n",
        "\n",
        "scorer_chain = (\n",
        "    genc.interop.langchain.CustomChain()\n",
        "    | scorer_model\n",
        "    | regex_match\n",
        "  )\n",
        "\n",
        "scorer_chain_ir = genc.interop.langchain.create_computation(scorer_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ebql2lF8ZG"
      },
      "source": [
        "Now, it's time to put all these pieces together into a whole. As noted before,\n",
        "we're going to illustrate here the use of GenC's conditional expression (one of\n",
        "the supplied operators) for composition. The expression first uses the scorer\n",
        "logic we defined just above, and depending on the outcome, passes control to\n",
        "either of the two conditional branches, each of which contains a call to one of\n",
        "the two models, for further evaluation.\n",
        "\n",
        "Once again, we generate th IR, so that we can deploy the result or Android."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5FjkSQbGY8h"
      },
      "outputs": [],
      "source": [
        "portable_ir = genc.authoring.create_lambda_from_fn(\n",
        "    \"x\",\n",
        "    lambda arg: genc.authoring.create_conditional(\n",
        "        genc.authoring.create_call(scorer_chain_ir, arg),\n",
        "        genc.authoring.create_call(my_cloud_model_ir, arg),\n",
        "        genc.authoring.create_call(my_on_device_model_ir, arg),\n",
        "    ),\n",
        ")\n",
        "print(portable_ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cssG2tkPHc4c"
      },
      "source": [
        "## Save the IR to a file\n",
        "\n",
        "The rest of the process looks just like in the proceeding tutorial. Run the\n",
        "following code to generate the a file containing the IR from above computation,\n",
        "and load it on the phone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY1DlYqnJ9XI"
      },
      "outputs": [],
      "source": [
        "with open(\"/tmp/genc_demo.pb\", \"wb\") as f:\n",
        "  f.write(portable_ir.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fk_BJZ6K5V3"
      },
      "source": [
        "## Install Generative Computing Demo app and deploy IR file to phone\n",
        "\n",
        "Please see the section entitled \"Install Generative Computing Demo app, deploy the IR file on the phone\" in\n",
        "[Tutorial 1](https://github.com/google/generative_computing/tree/master/generative_computing/docs/tutorials/tutorial_1_simple_cascade.ipynb)\n",
        "for detailed instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbkrAZ9boui"
      },
      "source": [
        "## Run the demo\n",
        "\n",
        "1. Open Generative Computing Demo app, and enter a sensitive query. See response.\n",
        "\n",
        "2. Next, enter a non-sensitive query. See response.\n",
        "\n",
        "3. Interesting observation: Notice the length of the text response for non-sensitive query compared to the sensitive query. Any guesses?\n",
        "  *  The non-sensitive query is routed to on-device model which in our demo setup returns shorter text responses to balance response time against lesser compute resources on device while the cloud model returns more verbose response.\n",
        "\n",
        "4. The intent of this custom routing example is to illustrate that developers can create custom routing policies to dynamically choose which model to use per\n",
        "different use-cases and model capabilities."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
